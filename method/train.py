# XNES+Adam.pyï¼Œæ–°æ–¹æ³•ï¼ŒåŒ…å«lr2 â€”â€” æŒ‰å…¨ç¨‹æœ€å°lossä¿å­˜æ¨¡å‹(center)
import os, sys

PROJECT_ROOT = "/home/chenfanke/TaskPINN"
os.environ.pop("PYTHONPATH", None)
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

# --- imports ---
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from jax import random, numpy as jnp
from jax.scipy.linalg import expm
from jax import jit
from flax.core.frozen_dict import unfreeze, freeze
from flax import serialization
from pathlib import Path

from src.utils import multi_SimManager

PROJECT_ROOT = Path("/home/chenfanke/TaskPINN")
TARGET_BASE = PROJECT_ROOT / "train" / "XNES+Adam_new"
LOSS_DIR = TARGET_BASE / "loss_iters"
RESULT_DIR = TARGET_BASE / "result"
LOSS_TIME = TARGET_BASE / "loss_time_csv"
PARAMS_DIR = TARGET_BASE / "params"
LOSS_DIR.mkdir(parents=True, exist_ok=True)
RESULT_DIR.mkdir(parents=True, exist_ok=True)
LOSS_TIME.mkdir(parents=True, exist_ok=True)
PARAMS_DIR.mkdir(parents=True, exist_ok=True)

# ===== é€‰æ‹© PDEï¼ˆåªæ”¹è¿™é‡Œï¼‰=====
pde = "Burgers1D"
from src.PDE_category.Burgers import Burgers1D

# =================================

# ===== å‘½åé…ç½®ï¼ˆæ–¹ç¨‹å_æ–¹æ³•_ç½‘ç»œå‚æ•°ï¼‰=====
method_name = "XNES+Adam_new"  # å»ºè®®ä¸ç›®å½•ä¸€è‡´
net_arch = "4*8"  # æŒ‰ä½ çš„ç½‘ç»œç»“æ„å¡«å†™

seed = 1

train_task = Burgers1D.PDE(hidden_layers='8*4')
policy = Burgers1D.PINNsPolicy(train_task.net,
                               train_task.num_params,
                               train_task.format_params_fn,
                               train_task.layout)

sim_mgr = multi_SimManager(n_repeats=1, test_n_repeats=1, pop_size=0, n_evaluations=1,
                           policy_net=policy, train_vec_task=train_task, valid_vec_task=train_task,
                           seed=0)


def get_fitness(sim_mgr, samples):
    losses, _ = sim_mgr.eval_params(params=samples, test=False)
    scores = jnp.sum(losses, axis=1)
    return -losses, scores


# åŸå§‹lr=1e-2
def xNES_Adam(f, x0, *, bs=100, lr=1e-2, lr2=0.003, sigma=1e-3, max_iters=10_000,
              beta1=0.9, beta2=0.999, eps=1e-8, verbose=True):
    key, rng = random.split(random.PRNGKey(seed))

    w0 = jnp.zeros(policy.num_params)
    center = w0.copy()
    dim = int(center.shape[0])
    I = jnp.eye(dim)
    A = I * sigma

    # Adam moments
    m = jnp.zeros(dim)
    v = jnp.zeros(dim)
    t_adam = 0

    bestFitness = -jnp.inf
    bestFound = None

    loss_ls = []
    various_loss_ls = []
    iter_time_ls = []
    runtime = 0.0

    @jit
    def project_sample(center, A, rng):
        key, rng = random.split(rng)
        samples = random.normal(key, (bs, dim))
        samples_o = samples @ A + center
        return samples, samples_o, rng

    @jit
    def compute_utilities(fitnesses):
        order = jnp.argsort(fitnesses)                 # å‡åºç´¢å¼•
        ranks = jnp.argsort(order).astype(jnp.float32) # 0..bs-1
        L = fitnesses.size
        u_raw = jnp.log(L / 2.0 + 1.0) - jnp.log(L - ranks)
        utilities = jnp.maximum(0.0, u_raw)
        utilities = utilities / jnp.sum(utilities)
        utilities = utilities - 1.0 / L
        return utilities

    @jit
    def update(center, A, m, v, t_adam, utilities, samples):
        # è‡ªç„¶æ¢¯åº¦ä¸­å¿ƒæ–¹å‘
        grad_c = A @ (utilities @ samples)

        # Adam æ›´æ–°ï¼ˆå­¦ä¹ ç‡ç”¨ lr2ï¼‰
        m = beta1 * m + (1.0 - beta1) * grad_c
        v = beta2 * v + (1.0 - beta2) * (grad_c ** 2)
        t_adam += 1
        m_hat = m / (1.0 - beta1 ** t_adam)
        v_hat = v / (1.0 - beta2 ** t_adam)
        center = center + lr2 * m_hat / (jnp.sqrt(v_hat) + eps)

        # åæ–¹å·®æ›´æ–°ï¼ˆå…¨çŸ©é˜µï¼‰
        covGrad = jnp.sum(
            utilities[:, None, None] *
            (samples[:, :, None] * samples[:, None, :] - I),
            axis=0
        )
        A = A @ expm(0.5 * lr * covGrad)
        return center, A, m, v, t_adam

    numEvals = 0
    for it in range(max_iters):
        t0 = time.time()

        samples, samples_o, rng = project_sample(center, A, rng)
        losses, fitnesses = f(sim_mgr, samples_o)  # shape [bs]


        avg_fit = float(jnp.mean(fitnesses))
        loss_iter = -avg_fit
        various_loss = np.mean(np.array(losses, copy=False), axis=0)

        loss_ls.append(loss_iter)
        various_loss_ls.append(various_loss)

        cur_best = float(jnp.max(fitnesses))
        if cur_best > float(bestFitness):
            idx = int(jnp.argmax(fitnesses))
            bestFitness = cur_best
            bestFound = np.array(samples_o[idx])
            # params_tree_best = flat_to_params_tree(center)
            # save_path = save_best_params(params_tree_best)
            # if verbose:
            #     print(f"ğŸ’¾ [iter {it+1}] æ–° best loss ä¿å­˜ï¼šloss={best_loss:.4e} â†’ {save_path}")

        numEvals += bs

        if verbose:
            runtime += (time.time() - t0)
            print(f"iter={it + 1:5d}  time={runtime:6.2f}s  loss={loss_iter:.2e}")

        utilities = compute_utilities(fitnesses)
        center, A, m, v, t_adam = update(center, A, m, v, t_adam, utilities, samples)

        iter_time_ls.append(float(time.time() - t0))

    print(f"\nFinished at iter={max_iters}, last loss={loss_ls[-1]:.2e}, best loss={bestFitness:.2e}")
    # ä¸ºå…¼å®¹å¤–éƒ¨å˜é‡åï¼Œç¬¬äºŒä¸ªè¿”å›å€¼ç»™ -best_lossï¼ˆç›¸å½“äºâ€œæœ€ä½³å¹³å‡fitnessâ€ï¼‰
    return bestFound, bestFitness, numEvals, iter_time_ls, loss_ls


# ===== è®­ç»ƒï¼ˆå›ºå®š 5,000 æ¬¡ï¼›å…¶ä½™å‚æ•°ä¿ç•™åŸé£æ ¼ï¼‰=====
max_iters = 50
w0 = jnp.zeros(policy.num_params)
best_w, best_fit, evals, iter_time_ls, loss_ls = xNES_Adam(
    get_fitness, w0,
    bs=100, lr=1e-2, lr2=0.003, sigma=0.001,
    max_iters=max_iters,
    beta1=0.9, beta2=0.999, eps=1e-8,
    verbose=True
)

# # ===== ä¿å­˜ loss æ›²çº¿åˆ° loss_iters/ =====
# fig_path = LOSS_DIR / f"{pde}_loss_iter.png"
# plt.figure(figsize=(10, 6))
# plt.plot(range(1, len(loss_ls) + 1), loss_ls, 'b-', linewidth=2)
# plt.xlabel('Iteration', fontsize=12)
# plt.ylabel('Loss', fontsize=12)
# plt.title(f'{pde} Loss Curve', fontsize=14)
# plt.grid(True, linestyle='--', alpha=0.7)
# plt.tight_layout()
# plt.savefig(fig_path, dpi=300, bbox_inches='tight')
# plt.close()
# print(f"ğŸ“ˆ æ›²çº¿å·²ä¿å­˜ï¼š{fig_path}")
#
# # ===== æ¢å¤æœ€ä¼˜å‚æ•° â†’ åšé¢„æµ‹ =====
# flat_best = jnp.array([best_w])
# this_dict = policy.format_params_fn(flat_best)
# new_dict = unfreeze(this_dict)
# for m in new_dict:
#     for p in new_dict[m]:
#         for k in new_dict[m][p]:
#             new_dict[m][p][k] = new_dict[m][p][k][0]
# params_tree = freeze(new_dict)
#
# X_input = train_task.X_candidate
# Y_true  = train_task.u_ref
# model   = train_task.net
# derivs  = model.derivatives(params_tree, X_input)
#
# # ===== ç»“æœ CSV ä¿å­˜åˆ° result/ =====
# if Y_true.shape[1] == 1:
#     u_pred = np.asarray(derivs['u'])
#     df = pd.DataFrame({
#         'x': X_input[:, 0],
#         'y': X_input[:, 1] if X_input.shape[1] >= 2 else 0,
#         't': X_input[:, 2] if X_input.shape[1] >= 3 else 0,
#         'u_true': Y_true[:, 0],
#         'u_pred': u_pred[:, 0],
#     })
# elif Y_true.shape[1] == 2:
#     u_pred = np.asarray(derivs['u'])
#     v_pred = np.asarray(derivs['v'])
#     df = pd.DataFrame({
#         'x': X_input[:, 0],
#         'y': X_input[:, 1],
#         't': X_input[:, 2] if X_input.shape[1] == 3 else 0,
#         'u_true': Y_true[:, 0],
#         'v_true': Y_true[:, 1],
#         'u_pred': u_pred[:, 0],
#         'v_pred': v_pred[:, 0],
#     })
# elif Y_true.shape[1] == 3:
#     u_pred = np.asarray(derivs['u'])
#     v_pred = np.asarray(derivs['v'])
#     p_pred = np.asarray(derivs['p'])
#     df = pd.DataFrame({
#         'x': X_input[:, 0],
#         'y': X_input[:, 1],
#         't': X_input[:, 2],
#         'u_true': Y_true[:, 0],
#         'v_true': Y_true[:, 1],
#         'p_true': Y_true[:, 2],
#         'u_pred': u_pred[:, 0],
#         'v_pred': v_pred[:, 0],
#         'p_pred': p_pred[:, 0],
#     })
# else:
#     raise ValueError(f"Unsupported output dimension: {Y_true.shape[1]}")
#
# csv_path = RESULT_DIR / f"{pde}_Result.csv"
# df.to_csv(csv_path, index=False)
# print(f"âœ… æ•°æ®å·²ä¿å­˜ï¼š{csv_path}")
#
# iter_time_cumsum = np.cumsum(iter_time_ls)
# df_log = pd.DataFrame({
#     "iter": np.arange(1, len(loss_ls) + 1, dtype=int),
#     "cum_time": iter_time_cumsum,
#     "loss": loss_ls
# })
# loss_time_csv_path = LOSS_TIME / f"{pde}_IterTime_Loss.csv"
# df_log.to_csv(loss_time_csv_path, index=False)
# print(f"âœ… ç´¯è®¡è€—æ—¶ä¸æŸå¤±å·²ä¿å­˜ï¼š{loss_time_csv_path}")
#
# # ===== ç»“æŸæç¤ºï¼šæœ€ä¼˜å‚æ•°æ–‡ä»¶è·¯å¾„ =====
# final_param_file = PARAMS_DIR / f"{pde}_{method_name}_{net_arch}.msgpack"
# print(f"ğŸ¯ è®­ç»ƒæœ€ä¼˜å‚æ•°å·²å­˜ä¸ºï¼š{final_param_file}")
